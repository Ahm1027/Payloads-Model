{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Payload_Model_Ngrams.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzPI9zcMXCtn0ZDzpF93J8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahm1027/Payloads-Model/blob/master/Payload_Model_Ngrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPnUtvD1hqtl"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import binascii"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFlRLYKeh3L-",
        "outputId": "017414d3-8956-4f17-e36a-fe0e8feada75"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy8opbw2jz6A"
      },
      "source": [
        "globalFeatures = np.array([])"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgPCOVs8zNzT",
        "outputId": "7b2e3f75-a55d-479f-8891-a0c6a519522c"
      },
      "source": [
        "def resolveMalwareDirs(FOLDER_PATHS, num_of_features=2000):\n",
        "  dict = {}\n",
        "  for files in os.listdir(FOLDER_PATHS):\n",
        "    filepath = r\"/content/drive/MyDrive/Payloads/\"+files\n",
        "    data = get_hexdump(filepath)\n",
        "    data_vectorizer, data_vectorized = vectorize(data)\n",
        "    data_sorted = arrangeAndSort(data_vectorizer, data_vectorized)[:num_of_features]\n",
        "    return mergeWithGlobalVocab(data_sorted, globalFeatures)\n",
        "\n",
        "globalFeatures = resolveMalwareDirs(r\"/content/drive/MyDrive/Payloads\")\n",
        "globalFeatures"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2000\n",
            "2000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['20202020', '6c617373', '636c6173', ..., '31253735', '3127292c',\n",
              "        '31272c20'],\n",
              "       ['937', '578', '577', ..., '12', '12', '12']], dtype='<U32')"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgDy9sURJLKP"
      },
      "source": [
        "globalFeatures"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Zm9Sc5p9dj"
      },
      "source": [
        "def get_hexdump(executable):\n",
        "  executable = open(executable, 'rb')\n",
        "  data = executable.read()\n",
        "  executable.close()\n",
        "  data2hex = bin2hex(data)\n",
        "  output = generate_ngrams(text=data2hex, WordsToCombine=4)\n",
        "  return output\n",
        "\n",
        "def bin2hex(data):\n",
        "  data2hex = \"\"\n",
        "  for byte in data:\n",
        "    hex = str(binascii.hexlify(bytes([byte]))).split(\"'\")\n",
        "    if (len(hex) >= 2):\n",
        "      data2hex += hex[1]+\" \"\n",
        "    else:\n",
        "      data2hex += hex[0]+\" \"\n",
        "  return data2hex\n",
        "\n",
        "def generate_ngrams(text, WordsToCombine):\n",
        "    words = text.split()\n",
        "    output = []  \n",
        "    for i in range(len(words)- WordsToCombine+1):\n",
        "      word = ''\n",
        "      for i in range (i, i+WordsToCombine):\n",
        "        word = word + words[i]\n",
        "      output.append(word)\n",
        "    return output\n",
        "\n",
        "# def calculateTotalCounts(uniqueOutput, output):\n",
        "#   Uniquecounts = []\n",
        "#   for i in range(len(uniqueOutput)):\n",
        "#     count = 0\n",
        "#     for j in range(len(output)):\n",
        "#       if output[j] == uniqueOutput[i]:\n",
        "#         count = count+1\n",
        "#     Uniquecounts.append(count)\n",
        "#   return Uniquecounts"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GulJOgg2qGL1"
      },
      "source": [
        "def vectorize(data):\n",
        "  vectorizer = CountVectorizer()\n",
        "  vectorizer.fit(data)\n",
        "  vectorized = vectorizer.transform(data)\n",
        "  return vectorizer, vectorized"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXiySbPlWRIW"
      },
      "source": [
        "def arrangeAndSort(data_vectorizer, data_vectorized):\n",
        "  data_arranged = list(zip(data_vectorizer.get_feature_names(), data_vectorized.sum(0).getA1()))\n",
        "  data_sorted = sorted(temp, key=lambda item: item[1], reverse=True)\n",
        "  return data_sorted"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9oAIsgHOmx_"
      },
      "source": [
        "def normalizeFeatures(data):\n",
        "  data_scaled = (data - data.min()) / (data.max() - data.min())\n",
        "  return data_scaled"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CLPEklEkT8o"
      },
      "source": [
        "def find_first_no_jit(item, vec):\n",
        "  for i in range(len(vec)):\n",
        "    if item == vec[i]:\n",
        "      return i\n",
        "  return -1\n",
        "\n",
        "def mergeWithGlobalVocab(data, globalFeatures):\n",
        "  if (len(globalFeatures) == 0):\n",
        "    globalFeatures = np.concatenate([globalFeatures, np.array(data)[:, 0].astype(str).tolist()])\n",
        "  else:\n",
        "    globalFeatures[0] = np.concatenate([globalFeatures, np.array(data)[:, 0].astype(str).tolist()])\n",
        "    globalFeatures[0], indexes = np.unique(globalFeatures, return_index=True)\n",
        "    globalFeatures[0] = globalFeatures[np.sort(indexes)]\n",
        "  features = [0]*len(globalFeatures)\n",
        "  for x in data:\n",
        "    id = find_first_no_jit(x[0], globalFeatures)\n",
        "    if (id != -1):\n",
        "      features[id] = x[1]\n",
        "  globalFeatures = np.vstack((globalFeatures, features))\n",
        "  return globalFeatures\n"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZGvdVtcqUUK"
      },
      "source": [
        "compressed_SVD = TruncatedSVD(30)\n",
        "compressed_data = compressed_SVD.fit_transform(features_frequency)\n",
        "pd.DataFrame(compressed_SVD.components_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPGNSrMvVf-j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}